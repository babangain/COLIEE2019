{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "directory = os.getcwd() \n",
    "import nltk\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  0\n",
      "Processing  100\n"
     ]
    }
   ],
   "source": [
    "# Extract text from inside\n",
    "\n",
    "def preprocess(folder):\n",
    "    \n",
    "    current_dir = directory + \"/\"+folder\n",
    "    base_cases = []\n",
    "    entailed_fragments = []\n",
    "    is_entailing = []\n",
    "    paragraphs = []\n",
    "    list_of_files = sorted(os.listdir(current_dir))\n",
    "    \n",
    "    for i in range(len(list_of_files)-2):\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing \",i)\n",
    "        f = open(current_dir+'/'+list_of_files[i]+\"/base_case.txt\",\"r\")\n",
    "        g = open(current_dir+'/'+list_of_files[i]+\"/entailed_fragment.txt\",\"r\")\n",
    "        h = open(current_dir+'/'+list_of_files[i]+\"/entailing_paragraphs.txt\",\"r\")\n",
    "        paragraph_filenames = sorted(os.listdir(current_dir+'/'+list_of_files[i]+\"/paragraphs\"))\n",
    "        tmp = list(map(int,h.read().split(\"\\n\")[:-1]))\n",
    "        a = f.read()\n",
    "        b = g.read()\n",
    "        for j in range(len(paragraph_filenames)):\n",
    "            base_cases.append(a)\n",
    "            entailed_fragments.append(b)\n",
    "        f.close()\n",
    "        g.close()\n",
    "        h.close()\n",
    "        \n",
    "        arr = []\n",
    "        for j in range(len(paragraph_filenames)):\n",
    "            if j+1 in tmp:\n",
    "                is_entailing.append(1)\n",
    "            else:\n",
    "                is_entailing.append(0)\n",
    "        for j in range(len(paragraph_filenames)):\n",
    "            g = open(current_dir+'/'+list_of_files[i]+\"/paragraphs/\"+paragraph_filenames[j],\"r\")\n",
    "            paragraphs.append(g.read())\n",
    "            g.close()\n",
    "    return base_cases,entailed_fragments,paragraphs,is_entailing\n",
    "    \n",
    "train_base_cases,train_entailed_fragments, train_paragraphs,train_is_entailing = preprocess('task2_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.column_stack([train_entailed_fragments,train_paragraphs,\n",
    "                                      train_is_entailing]),columns=['question1','question2','is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.dropna(how=\"any\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0 \n",
    "for i in range(a,a+10):\n",
    "    print(df.question1[i])\n",
    "    print(df.question2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = 'What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?'\n",
    "question2 = 'How will a Trump presidency affect the students presently in US or planning to study in US?'\n",
    "\n",
    "question1 = question1.lower().split()\n",
    "question2 = question2.lower().split()\n",
    "\n",
    "question1 = [w for w in question1 if w not in stop_words]\n",
    "question2 = [w for w in question2 if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Model\n",
    "data = df.question1.values + df.question2.values\n",
    "model = gensim.models.Word2Vec(data,size=150,window=5,min_count=1,workers=10)\n",
    "model.train(data, total_examples=len(data), epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 18:53:05,294 : INFO : loading Word2VecKeyedVectors object from quora.bin.gz\n",
      "2019-03-04 18:53:05,448 : INFO : loading wv recursively from quora.bin.gz.wv.* with mmap=None\n",
      "2019-03-04 18:53:05,450 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-04 18:53:05,452 : INFO : loading vocabulary recursively from quora.bin.gz.vocabulary.* with mmap=None\n",
      "2019-03-04 18:53:05,454 : INFO : loading trainables recursively from quora.bin.gz.trainables.* with mmap=None\n",
      "2019-03-04 18:53:05,456 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-04 18:53:05,457 : INFO : loaded quora.bin.gz\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"train.bin.gz\")\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "    \n",
    "model = gensim.models.KeyedVectors.load('quora.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    return model.wmdistance(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'qid1', 'qid2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_q1\n",
      "len_q2\n",
      "len_char_q1\n",
      "len_char_q2\n",
      "len_ wordq1\n"
     ]
    }
   ],
   "source": [
    "df['len_q1'] = df.question1.apply(lambda x: len(str(x)))\n",
    "print(\"len_q1\")\n",
    "df['len_q2'] = df.question2.apply(lambda x: len(str(x)))\n",
    "print(\"len_q2\")\n",
    "df['diff_len'] = df.len_q1 - df.len_q2\n",
    "df['len_char_q1'] = df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "print(\"len_char_q1\")\n",
    "df['len_char_q2'] = df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "print(\"len_char_q2\")\n",
    "df['len_word_q1'] = df.question1.apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df.question2.apply(lambda x: len(str(x).split()))\n",
    "print(\"len_ wordq1\")\n",
    "df['common_words'] = df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "df['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['question1', 'question2'], axis=1, inplace=True)\n",
    "\n",
    "#X = df.loc[:, df.columns != 'is_duplicate']\n",
    "#y = df.loc[:, df.columns == 'is_duplicate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72378  4269]\n",
      " [ 4438 40202]]\n",
      "Accuracy 0.9282115972857767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     76647\n",
      "           1       0.90      0.90      0.90     44640\n",
      "\n",
      "   micro avg       0.93      0.93      0.93    121287\n",
      "   macro avg       0.92      0.92      0.92    121287\n",
      "weighted avg       0.93      0.93      0.93    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, \n",
    "                          gamma=0, reg_alpha=4, objective='binary:logistic', \n",
    "                          eta=0.3, verbosity=3, subsample=0.8).fit(X, y.values.ravel()) \n",
    "prediction = model.predict(X_test)\n",
    "cm = confusion_matrix( y_test, prediction ) \n",
    "print(cm)  \n",
    "print('Accuracy', accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "0 25\n",
      "0 26\n",
      "0 27\n",
      "0 28\n",
      "0 29\n",
      "0 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "0 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "0 39\n",
      "0 40\n",
      "0 41\n",
      "0 42\n",
      "0 43\n",
      "0 44\n",
      "0 45\n",
      "0 46\n",
      "0 47\n",
      "0 48\n",
      "0 49\n",
      "0 50\n",
      "0 51\n",
      "0 52\n",
      "0 53\n",
      "0 54\n",
      "0 55\n",
      "0 56\n",
      "0 57\n",
      "0 58\n",
      "0 59\n",
      "0 60\n",
      "0 61\n",
      "0 62\n",
      "0 63\n",
      "0 64\n",
      "0 65\n",
      "0 66\n",
      "0 67\n",
      "0 68\n",
      "0 69\n",
      "0 70\n",
      "0 71\n",
      "0 72\n",
      "0 73\n",
      "0 74\n",
      "0 75\n",
      "0 76\n",
      "0 77\n",
      "0 78\n",
      "0 79\n",
      "0 80\n",
      "0 81\n",
      "0 82\n",
      "0 83\n",
      "0 84\n",
      "0 85\n",
      "0 86\n",
      "0 87\n",
      "0 88\n",
      "0 89\n",
      "0 90\n",
      "0 91\n",
      "0 92\n",
      "0 93\n",
      "0 94\n",
      "0 95\n",
      "0 96\n",
      "0 97\n",
      "0 98\n",
      "0 99\n",
      "0 100\n",
      "0 101\n",
      "0 102\n",
      "0 103\n",
      "0 104\n",
      "0 105\n",
      "0 106\n",
      "0 107\n",
      "0 108\n",
      "0 109\n",
      "0 110\n",
      "0 111\n",
      "0 112\n",
      "0 113\n",
      "0 114\n",
      "0 115\n",
      "0 116\n",
      "0 117\n",
      "0 118\n",
      "0 119\n",
      "0 120\n",
      "0 121\n",
      "0 122\n",
      "0 123\n",
      "0 124\n",
      "0 125\n",
      "0 126\n",
      "0 127\n",
      "0 128\n",
      "0 129\n",
      "0 130\n",
      "0 131\n",
      "0 132\n",
      "0 133\n",
      "0 134\n",
      "0 135\n",
      "0 136\n",
      "0 137\n",
      "0 138\n",
      "0 139\n",
      "0 140\n",
      "0 141\n",
      "0 142\n",
      "0 143\n",
      "0 144\n",
      "0 145\n",
      "0 146\n",
      "0 147\n",
      "0 148\n",
      "0 149\n",
      "0 150\n",
      "0 151\n",
      "0 152\n",
      "0 153\n",
      "0 154\n",
      "0 155\n",
      "0 156\n",
      "0 157\n",
      "0 158\n",
      "0 159\n",
      "0 160\n",
      "0 161\n",
      "0 162\n",
      "0 163\n",
      "0 164\n",
      "0 165\n",
      "0 166\n",
      "0 167\n",
      "0 168\n",
      "0 169\n",
      "0 170\n",
      "0 171\n",
      "0 172\n",
      "0 173\n",
      "0 174\n",
      "0 175\n",
      "0 176\n",
      "0 177\n",
      "0 178\n",
      "0 179\n",
      "0 180\n",
      "0 181\n",
      "0 182\n",
      "0 183\n",
      "0 184\n",
      "0 185\n",
      "0 186\n",
      "0 187\n",
      "0 188\n",
      "0 189\n",
      "0 190\n",
      "0 191\n",
      "0 192\n",
      "0 193\n",
      "0 194\n",
      "0 195\n",
      "0 196\n",
      "0 197\n",
      "0 198\n",
      "0 199\n",
      "0 200\n",
      "0 201\n",
      "0 202\n",
      "0 203\n",
      "0 204\n",
      "0 205\n",
      "0 206\n",
      "0 207\n",
      "0 208\n",
      "0 209\n",
      "0 210\n",
      "0 211\n",
      "0 212\n",
      "0 213\n",
      "0 214\n",
      "0 215\n",
      "0 216\n",
      "0 217\n",
      "1 218\n",
      "0 219\n",
      "0 220\n",
      "0 221\n",
      "0 222\n",
      "0 223\n",
      "0 224\n",
      "0 225\n",
      "0 226\n",
      "0 227\n",
      "0 228\n",
      "0 229\n",
      "0 230\n",
      "0 231\n",
      "0 232\n",
      "0 233\n",
      "0 234\n",
      "0 235\n",
      "0 236\n",
      "0 237\n",
      "0 238\n",
      "0 239\n",
      "0 240\n",
      "0 241\n",
      "0 242\n",
      "0 243\n",
      "0 244\n",
      "0 245\n",
      "0 246\n",
      "0 247\n",
      "0 248\n",
      "0 249\n",
      "0 250\n",
      "0 251\n",
      "0 252\n",
      "0 253\n",
      "0 254\n",
      "0 255\n",
      "0 256\n",
      "0 257\n",
      "0 258\n",
      "0 259\n",
      "0 260\n",
      "0 261\n",
      "0 262\n",
      "0 263\n",
      "0 264\n",
      "0 265\n",
      "0 266\n",
      "0 267\n",
      "0 268\n",
      "0 269\n",
      "0 270\n",
      "0 271\n",
      "0 272\n",
      "0 273\n",
      "0 274\n",
      "0 275\n",
      "0 276\n",
      "0 277\n",
      "0 278\n",
      "0 279\n",
      "0 280\n",
      "0 281\n",
      "0 282\n",
      "0 283\n",
      "0 284\n",
      "0 285\n",
      "0 286\n",
      "0 287\n",
      "0 288\n",
      "0 289\n",
      "0 290\n",
      "0 291\n",
      "0 292\n",
      "0 293\n",
      "0 294\n",
      "0 295\n",
      "0 296\n",
      "0 297\n",
      "0 298\n",
      "0 299\n",
      "0 300\n",
      "0 301\n",
      "0 302\n",
      "0 303\n",
      "0 304\n",
      "0 305\n",
      "0 306\n",
      "0 307\n",
      "0 308\n",
      "0 309\n",
      "0 310\n",
      "0 311\n",
      "0 312\n",
      "0 313\n",
      "0 314\n",
      "0 315\n",
      "0 316\n",
      "0 317\n",
      "0 318\n",
      "0 319\n",
      "0 320\n",
      "0 321\n",
      "0 322\n",
      "0 323\n",
      "0 324\n",
      "0 325\n",
      "0 326\n",
      "0 327\n",
      "0 328\n",
      "0 329\n",
      "0 330\n",
      "0 331\n",
      "0 332\n",
      "0 333\n",
      "0 334\n",
      "0 335\n",
      "0 336\n",
      "0 337\n",
      "0 338\n",
      "0 339\n",
      "0 340\n",
      "0 341\n",
      "0 342\n",
      "0 343\n",
      "0 344\n",
      "0 345\n",
      "0 346\n",
      "0 347\n",
      "0 348\n",
      "0 349\n",
      "0 350\n",
      "0 351\n",
      "0 352\n",
      "0 353\n",
      "0 354\n",
      "0 355\n",
      "0 356\n",
      "0 357\n",
      "0 358\n",
      "0 359\n",
      "0 360\n",
      "0 361\n",
      "0 362\n",
      "0 363\n",
      "0 364\n",
      "0 365\n",
      "0 366\n",
      "0 367\n",
      "0 368\n",
      "0 369\n",
      "0 370\n",
      "0 371\n",
      "0 372\n",
      "0 373\n",
      "0 374\n",
      "0 375\n",
      "0 376\n",
      "0 377\n",
      "0 378\n",
      "0 379\n",
      "0 380\n",
      "0 381\n",
      "0 382\n",
      "0 383\n",
      "0 384\n",
      "0 385\n",
      "0 386\n",
      "0 387\n",
      "0 388\n",
      "0 389\n",
      "0 390\n",
      "0 391\n",
      "0 392\n",
      "0 393\n",
      "0 394\n",
      "0 395\n",
      "0 396\n",
      "0 397\n",
      "0 398\n",
      "0 399\n",
      "0 400\n",
      "0 401\n",
      "0 402\n",
      "0 403\n",
      "0 404\n",
      "0 405\n",
      "0 406\n",
      "0 407\n",
      "0 408\n",
      "0 409\n",
      "0 410\n",
      "0 411\n",
      "0 412\n",
      "0 413\n",
      "0 414\n",
      "0 415\n",
      "0 416\n",
      "0 417\n",
      "0 418\n",
      "0 419\n",
      "0 420\n",
      "0 421\n",
      "0 422\n",
      "0 423\n",
      "0 424\n",
      "0 425\n",
      "0 426\n",
      "0 427\n",
      "0 428\n",
      "0 429\n",
      "0 430\n",
      "0 431\n",
      "0 432\n",
      "0 433\n",
      "0 434\n",
      "0 435\n",
      "0 436\n",
      "0 437\n",
      "0 438\n",
      "0 439\n",
      "0 440\n",
      "0 441\n",
      "0 442\n",
      "0 443\n",
      "0 444\n",
      "0 445\n",
      "0 446\n",
      "0 447\n",
      "0 448\n",
      "0 449\n",
      "0 450\n",
      "0 451\n",
      "0 452\n",
      "0 453\n",
      "0 454\n",
      "0 455\n",
      "0 456\n",
      "0 457\n",
      "0 458\n",
      "0 459\n",
      "0 460\n",
      "0 461\n",
      "0 462\n",
      "0 463\n",
      "0 464\n",
      "0 465\n",
      "0 466\n",
      "0 467\n",
      "0 468\n",
      "0 469\n",
      "0 470\n",
      "0 471\n",
      "0 472\n",
      "0 473\n",
      "0 474\n",
      "0 475\n",
      "0 476\n",
      "0 477\n",
      "0 478\n",
      "0 479\n",
      "0 480\n",
      "0 481\n",
      "0 482\n",
      "0 483\n",
      "0 484\n",
      "0 485\n",
      "0 486\n",
      "0 487\n",
      "0 488\n",
      "0 489\n",
      "0 490\n",
      "0 491\n",
      "0 492\n",
      "0 493\n",
      "0 494\n",
      "0 495\n",
      "0 496\n",
      "0 497\n",
      "0 498\n",
      "0 499\n",
      "0 500\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    print(prediction[i],i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5765   11]\n",
      " [   0    0]]\n",
      "Accuracy 0.998095567867036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5776\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      5776\n",
      "   macro avg       0.50      0.50      0.50      5776\n",
      "weighted avg       1.00      1.00      1.00      5776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_train)\n",
    "y_train = Y_trr\n",
    "cm = confusion_matrix( y_train, prediction ) \n",
    "print(cm)  \n",
    "print('Accuracy', accuracy_score(y_train, prediction))\n",
    "print(classification_report(y_train, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trr = []\n",
    "for i in range(len(y_train)):\n",
    "    Y_trr.append(int(y_train.values[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', 0], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
