{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "directory = os.getcwd() \n",
    "import nltk\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  0\n"
     ]
    }
   ],
   "source": [
    "# Extract text from inside\n",
    "\n",
    "def preprocess(folder):\n",
    "    no_of_candidates = []\n",
    "    no_of_paras = []\n",
    "    current_dir = directory + \"/\"+folder\n",
    "    base_cases = []\n",
    "    entailed_fragments = []\n",
    "    paragraphs = []\n",
    "    list_of_files = sorted(os.listdir(current_dir))\n",
    "    if folder == 'task2_train':\n",
    "        x=1\n",
    "    else:\n",
    "        x = 2\n",
    "    for i in range(len(list_of_files)-x):\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing \",i)\n",
    "        f = open(current_dir+'/'+list_of_files[i]+\"/base_case.txt\",\"r\")\n",
    "        g = open(current_dir+'/'+list_of_files[i]+\"/entailed_fragment.txt\",\"r\")\n",
    "        paragraph_filenames = sorted(os.listdir(current_dir+'/'+list_of_files[i]+\"/paragraphs\"))\n",
    "        no_of_candidates.append(len(paragraph_filenames))\n",
    "        a = f.read()\n",
    "        b = g.read()\n",
    "        base_cases.append(a)\n",
    "        entailed_fragments.append(b)\n",
    "        f.close()\n",
    "        g.close()\n",
    "        \n",
    "        arr = []\n",
    "        no_of_paras.append(len(paragraph_filenames))\n",
    "\n",
    "        for j in range(len(paragraph_filenames)):\n",
    "            g = open(current_dir+'/'+list_of_files[i]+\"/paragraphs/\"+paragraph_filenames[j],\"r\")\n",
    "            paragraphs.append(g.read())\n",
    "            g.close()\n",
    "    return base_cases,entailed_fragments,paragraphs,no_of_paras,no_of_candidates\n",
    "    \n",
    "#train_base_cases,train_entailed_fragments, train_paragraphs,train_no_of_paras = preprocess('task2_train')\n",
    "    \n",
    "test_base_cases,test_entailed_fragments, test_paragraphs,test_no_of_paras,no_of_candidates = preprocess('task2_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dict_words = set(nltk.corpus.words.words())\n",
    "def filt(case_docs):\n",
    "    filtered_words =[]\n",
    "    filtered_docs = [ '' for i in range(len(case_docs))]\n",
    "    lemma_docs = [ '' for i in range(len(case_docs))]\n",
    "    for i in range(len(case_docs)):\n",
    "        case_words = re.split(\"(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)\", case_docs[i])\n",
    "        filtered_word_list = [word.lower() for word in case_words if (( len(word) >= 3 and word.isalpha() and word.lower() not in stop_words ))  ]\n",
    "        filtered_words.append(filtered_word_list)\n",
    "        '''or ( len(word) <= 3 and word.isdigit())'''\n",
    "        for word in filtered_word_list:\n",
    "            filtered_docs[i] = filtered_docs[i] + word + \" \"\n",
    "            lemma_docs[i] = lemma_docs[i] + lemma.lemmatize(word) + \" \"\n",
    "    return lemma_docs\n",
    "test_entailed_fragments = filt(test_entailed_fragments)\n",
    "test_paragraphs = filt(test_paragraphs)\n",
    "test_base_cases = filt(test_base_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_entailed_fragments + test_paragraphs\n",
    "documents = test_entailed_fragments + test_paragraphs + test_base_cases\n",
    "def gen_labels(data):\n",
    "    labels = []\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        labels.append(\"base_case_\"+str(i+1))\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        for j in range(no_of_candidates[i]):\n",
    "            labels.append(\"candidate_\"+str(i+1)+\"_\"+str(j+1))\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        labels.append(\"extra_\"+str(i+1))\n",
    "    return labels\n",
    "labels = gen_labels(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dict_words = set(nltk.corpus.words.words())\n",
    "def filt(case_docs):\n",
    "    filtered_words =[]\n",
    "    filtered_docs = [ '' for i in range(len(case_docs))]\n",
    "    lemma_docs = [ '' for i in range(len(case_docs))]\n",
    "    for i in range(len(case_docs)):\n",
    "        #print(i)\n",
    "        case_words = re.split(\"(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)\", case_docs[i])\n",
    "        filtered_word_list = [word.lower() for word in case_words if (( len(word) >= 3 and word.isalpha() and word.lower() not in stop_words ))  ]\n",
    "        filtered_words.append(filtered_word_list)\n",
    "        '''or ( len(word) <= 3 and word.isdigit())'''\n",
    "        for word in filtered_word_list:\n",
    "            filtered_docs[i] = filtered_docs[i] + word + \" \"\n",
    "            lemma_docs[i] = lemma_docs[i] + lemma.lemmatize(word) + \" \"\n",
    "    return lemma_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "lst2 = test_entailed_fragments + test_paragraphs + test_base_cases\n",
    "lst3 = filt(lst2)\n",
    "arr = []\n",
    "for elem in lst3:\n",
    "    arr.append(elem.split())\n",
    "corpus = arr\n",
    "result = get_bm25_weights(corpus, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 20:18:13,669 : INFO : loading Doc2Vec object from doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model\n",
      "2019-03-30 20:18:13,789 : INFO : loading vocabulary recursively from doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model.vocabulary.* with mmap=None\n",
      "2019-03-30 20:18:13,790 : INFO : loading trainables recursively from doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model.trainables.* with mmap=None\n",
      "2019-03-30 20:18:13,790 : INFO : loading wv recursively from doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model.wv.* with mmap=None\n",
      "2019-03-30 20:18:13,791 : INFO : loading docvecs recursively from doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model.docvecs.* with mmap=None\n",
      "2019-03-30 20:18:13,792 : INFO : loaded doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model = Doc2Vec.load(\"doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1-1 48 IITPdocBM\n",
      "t1-2 154 IITPdocBM\n",
      "t1-3 180 IITPdocBM\n",
      "t1-3 124 IITPdocBM\n",
      "t1-4 137 IITPdocBM\n",
      "t1-4 182 IITPdocBM\n",
      "t1-4 65 IITPdocBM\n",
      "t1-5 174 IITPdocBM\n",
      "t1-5 163 IITPdocBM\n",
      "t1-5 130 IITPdocBM\n",
      "t1-5 110 IITPdocBM\n",
      "t1-5 111 IITPdocBM\n",
      "t1-6 124 IITPdocBM\n",
      "t1-6 62 IITPdocBM\n",
      "t1-6 85 IITPdocBM\n",
      "t1-6 168 IITPdocBM\n",
      "t1-6 187 IITPdocBM\n",
      "t1-6 184 IITPdocBM\n",
      "t1-6 45 IITPdocBM\n",
      "t1-6 167 IITPdocBM\n",
      "t1-6 58 IITPdocBM\n",
      "t1-6 70 IITPdocBM\n",
      "t1-7 178 IITPdocBM\n",
      "t1-7 22 IITPdocBM\n",
      "t1-8 88 IITPdocBM\n",
      "t1-8 157 IITPdocBM\n",
      "t1-8 182 IITPdocBM\n",
      "t1-9 71 IITPdocBM\n",
      "t1-10 180 IITPdocBM\n",
      "t1-10 187 IITPdocBM\n",
      "t1-10 51 IITPdocBM\n",
      "t1-10 154 IITPdocBM\n",
      "t1-11 11 IITPdocBM\n",
      "t1-11 111 IITPdocBM\n",
      "t1-11 25 IITPdocBM\n",
      "t1-11 105 IITPdocBM\n",
      "t1-11 2 IITPdocBM\n",
      "t1-11 49 IITPdocBM\n",
      "t1-11 195 IITPdocBM\n",
      "t1-12 7 IITPdocBM\n",
      "t1-12 124 IITPdocBM\n",
      "t1-12 50 IITPdocBM\n",
      "t1-12 194 IITPdocBM\n",
      "t1-12 11 IITPdocBM\n",
      "t1-12 192 IITPdocBM\n",
      "t1-12 52 IITPdocBM\n",
      "t1-13 95 IITPdocBM\n",
      "t1-14 156 IITPdocBM\n",
      "t1-14 79 IITPdocBM\n",
      "t1-15 18 IITPdocBM\n",
      "t1-16 124 IITPdocBM\n",
      "t1-16 58 IITPdocBM\n",
      "t1-17 127 IITPdocBM\n",
      "t1-17 124 IITPdocBM\n",
      "t1-18 177 IITPdocBM\n",
      "t1-18 136 IITPdocBM\n",
      "t1-19 81 IITPdocBM\n",
      "t1-19 120 IITPdocBM\n",
      "t1-19 11 IITPdocBM\n",
      "t1-19 101 IITPdocBM\n",
      "t1-19 59 IITPdocBM\n",
      "t1-20 180 IITPdocBM\n",
      "t1-20 11 IITPdocBM\n",
      "t1-20 76 IITPdocBM\n",
      "t1-21 151 IITPdocBM\n",
      "t1-21 166 IITPdocBM\n",
      "t1-21 83 IITPdocBM\n",
      "t1-21 42 IITPdocBM\n",
      "t1-21 114 IITPdocBM\n",
      "t1-22 173 IITPdocBM\n",
      "t1-22 6 IITPdocBM\n",
      "t1-22 88 IITPdocBM\n",
      "t1-22 163 IITPdocBM\n",
      "t1-22 189 IITPdocBM\n",
      "t1-22 5 IITPdocBM\n",
      "t1-23 125 IITPdocBM\n",
      "t1-23 14 IITPdocBM\n",
      "t1-23 164 IITPdocBM\n",
      "t1-24 127 IITPdocBM\n",
      "t1-24 104 IITPdocBM\n",
      "t1-24 99 IITPdocBM\n",
      "t1-25 5 IITPdocBM\n",
      "t1-25 95 IITPdocBM\n",
      "t1-25 91 IITPdocBM\n",
      "t1-26 104 IITPdocBM\n",
      "t1-26 114 IITPdocBM\n",
      "t1-27 116 IITPdocBM\n",
      "t1-28 80 IITPdocBM\n",
      "t1-28 191 IITPdocBM\n",
      "t1-28 165 IITPdocBM\n",
      "t1-28 130 IITPdocBM\n",
      "t1-29 22 IITPdocBM\n",
      "t1-29 15 IITPdocBM\n",
      "t1-29 19 IITPdocBM\n",
      "t1-30 9 IITPdocBM\n",
      "t1-30 44 IITPdocBM\n",
      "t1-31 84 IITPdocBM\n",
      "t1-31 66 IITPdocBM\n",
      "t1-32 134 IITPdocBM\n",
      "t1-33 109 IITPdocBM\n",
      "t1-33 26 IITPdocBM\n",
      "t1-34 121 IITPdocBM\n",
      "t1-34 63 IITPdocBM\n",
      "t1-35 2 IITPdocBM\n",
      "t1-35 167 IITPdocBM\n",
      "t1-36 145 IITPdocBM\n",
      "t1-36 51 IITPdocBM\n",
      "t1-36 180 IITPdocBM\n",
      "t1-37 33 IITPdocBM\n",
      "t1-37 165 IITPdocBM\n",
      "t1-38 43 IITPdocBM\n",
      "t1-38 14 IITPdocBM\n",
      "t1-38 94 IITPdocBM\n",
      "t1-38 114 IITPdocBM\n",
      "t1-38 28 IITPdocBM\n",
      "t1-38 142 IITPdocBM\n",
      "t1-39 40 IITPdocBM\n",
      "t1-39 50 IITPdocBM\n",
      "t1-40 128 IITPdocBM\n",
      "t1-41 147 IITPdocBM\n",
      "t1-41 153 IITPdocBM\n",
      "t1-41 195 IITPdocBM\n",
      "t1-42 83 IITPdocBM\n",
      "t1-42 198 IITPdocBM\n",
      "t1-42 13 IITPdocBM\n",
      "t1-42 192 IITPdocBM\n",
      "t1-42 65 IITPdocBM\n",
      "t1-42 179 IITPdocBM\n",
      "t1-43 175 IITPdocBM\n",
      "t1-43 13 IITPdocBM\n",
      "t1-44 11 IITPdocBM\n",
      "t1-44 99 IITPdocBM\n",
      "t1-44 92 IITPdocBM\n",
      "t1-45 45 IITPdocBM\n",
      "t1-45 111 IITPdocBM\n",
      "t1-45 7 IITPdocBM\n",
      "t1-45 132 IITPdocBM\n",
      "t1-45 189 IITPdocBM\n",
      "t1-45 151 IITPdocBM\n",
      "t1-45 41 IITPdocBM\n",
      "t1-45 187 IITPdocBM\n",
      "t1-46 23 IITPdocBM\n",
      "t1-46 102 IITPdocBM\n",
      "t1-46 185 IITPdocBM\n",
      "t1-47 82 IITPdocBM\n",
      "t1-47 49 IITPdocBM\n",
      "t1-47 160 IITPdocBM\n",
      "t1-48 37 IITPdocBM\n",
      "t1-48 179 IITPdocBM\n",
      "t1-48 72 IITPdocBM\n",
      "t1-48 114 IITPdocBM\n",
      "t1-48 123 IITPdocBM\n",
      "t1-48 21 IITPdocBM\n",
      "t1-49 8 IITPdocBM\n",
      "t1-50 150 IITPdocBM\n",
      "t1-50 125 IITPdocBM\n",
      "t1-50 58 IITPdocBM\n",
      "t1-50 64 IITPdocBM\n",
      "t1-50 194 IITPdocBM\n",
      "t1-51 199 IITPdocBM\n",
      "t1-51 58 IITPdocBM\n",
      "t1-52 9 IITPdocBM\n",
      "t1-53 39 IITPdocBM\n",
      "t1-54 177 IITPdocBM\n",
      "t1-54 114 IITPdocBM\n",
      "t1-54 159 IITPdocBM\n",
      "t1-54 12 IITPdocBM\n",
      "t1-54 31 IITPdocBM\n",
      "t1-54 168 IITPdocBM\n",
      "t1-54 11 IITPdocBM\n",
      "t1-54 107 IITPdocBM\n",
      "t1-54 140 IITPdocBM\n",
      "t1-54 7 IITPdocBM\n",
      "t1-55 188 IITPdocBM\n",
      "t1-55 147 IITPdocBM\n",
      "t1-55 117 IITPdocBM\n",
      "t1-56 116 IITPdocBM\n",
      "t1-56 53 IITPdocBM\n",
      "t1-56 7 IITPdocBM\n",
      "t1-56 184 IITPdocBM\n",
      "t1-56 12 IITPdocBM\n",
      "t1-56 71 IITPdocBM\n",
      "t1-57 87 IITPdocBM\n",
      "t1-57 130 IITPdocBM\n",
      "t1-58 6 IITPdocBM\n",
      "t1-58 54 IITPdocBM\n",
      "t1-58 22 IITPdocBM\n",
      "t1-58 14 IITPdocBM\n",
      "t1-58 83 IITPdocBM\n",
      "t1-59 27 IITPdocBM\n",
      "t1-59 92 IITPdocBM\n",
      "t1-59 113 IITPdocBM\n",
      "t1-59 165 IITPdocBM\n",
      "t1-60 168 IITPdocBM\n",
      "t1-60 34 IITPdocBM\n",
      "t1-60 188 IITPdocBM\n",
      "t1-60 109 IITPdocBM\n",
      "t1-61 181 IITPdocBM\n",
      "t1-61 33 IITPdocBM\n",
      "t1-61 131 IITPdocBM\n",
      "t1-61 134 IITPdocBM\n"
     ]
    }
   ],
   "source": [
    "for i in range(61):\n",
    "    \n",
    "    lst = []\n",
    "    for j in range(200):\n",
    "        a = model.docvecs.similarity(labels[i],\"candidate_\"+train_list_of_files[i]+\"_\"+str(j+1))\n",
    "        b = result[0][j+1]\n",
    "        lst.append((a*b,j+1))\n",
    "    lst.sort(reverse=True)\n",
    "    cutoff = ((lst[0][0] + lst[1][0])/2)*0.80\n",
    "    for j in range(10):\n",
    "        if lst[j][0] > cutoff:\n",
    "            print(\"t1-\"+str(i+1),lst[j][1],\"IITPdocBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2-1 1 IITP2BM25\n",
      "t2-2 12 IITP2BM25\n",
      "t2-3 18 IITP2BM25\n",
      "t2-4 15 IITP2BM25\n",
      "t2-5 59 IITP2BM25\n",
      "t2-6 25 IITP2BM25\n",
      "t2-7 27 IITP2BM25\n",
      "t2-8 7 IITP2BM25\n",
      "t2-9 14 IITP2BM25\n",
      "t2-10 5 IITP2BM25\n",
      "t2-11 24 IITP2BM25\n",
      "t2-12 12 IITP2BM25\n",
      "t2-13 20 IITP2BM25\n",
      "t2-14 13 IITP2BM25\n",
      "t2-15 42 IITP2BM25\n",
      "t2-16 7 IITP2BM25\n",
      "t2-17 31 IITP2BM25\n",
      "t2-18 13 IITP2BM25\n",
      "t2-19 7 IITP2BM25\n",
      "t2-20 26 IITP2BM25\n",
      "t2-21 58 IITP2BM25\n",
      "t2-22 24 IITP2BM25\n",
      "t2-23 16 IITP2BM25\n",
      "t2-24 16 IITP2BM25\n",
      "t2-25 47 IITP2BM25\n",
      "t2-26 17 IITP2BM25\n",
      "t2-27 16 IITP2BM25\n",
      "t2-28 21 IITP2BM25\n",
      "t2-29 34 IITP2BM25\n",
      "t2-30 29 IITP2BM25\n",
      "t2-31 24 IITP2BM25\n",
      "t2-32 24 IITP2BM25\n",
      "t2-33 8 IITP2BM25\n",
      "t2-34 26 IITP2BM25\n",
      "t2-35 7 IITP2BM25\n",
      "t2-36 18 IITP2BM25\n",
      "t2-37 12 IITP2BM25\n",
      "t2-38 13 IITP2BM25\n",
      "t2-39 12 IITP2BM25\n",
      "t2-40 31 IITP2BM25\n",
      "t2-41 15 IITP2BM25\n",
      "t2-42 20 IITP2BM25\n",
      "t2-43 16 IITP2BM25\n",
      "t2-44 25 IITP2BM25\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    #print(\"\\nPredicting  For \",i)\n",
    "    lst = []\n",
    "    for j in range(no_of_candidates[i]):\n",
    "        #lst.append((model.docvecs.similarity(labels[i],labels[285+i*200+j]),j+1))\n",
    "        a = result[i][44+sum(no_of_candidates[:i])+j]\n",
    "        b = model.docvecs.similarity(labels[i],\"candidate_\"+str(i+1)+\"_\"+str(j+1))\n",
    "        lst.append((a*b,j+1))\n",
    "        #print(labels[44+sum(no_of_candidates[:i])+j])\n",
    "    lst.sort(reverse=True)\n",
    "    cutoff = ((lst[0][0] + lst[1][0])/2)*0.90\n",
    "    for j in range(1):\n",
    "        if lst[j][0] > cutoff:\n",
    "            print(\"t2-\"+str(i+1),lst[j][1],\"IITP2BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(200):\n",
    "    lst.append((result[0][j+1],j+1))\n",
    "lst.sort(reverse=True)\n",
    "cutoff = ((lst[0][0] + lst[1][0])/2)*0.90\n",
    "for j in range(10):\n",
    "    if lst[j][0] > cutoff:\n",
    "        print(\"t1-\"+str(i+1),lst[j][1],\"IITPBM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-75020.47405382311"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "325,1486,855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "588/2421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "468/1486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = re.search(\"Baban\",\"I am Baban Gain Baban GAin\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
