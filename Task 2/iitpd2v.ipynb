{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "directory = os.getcwd() \n",
    "import nltk\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  0\n"
     ]
    }
   ],
   "source": [
    "# Extract text from inside\n",
    "\n",
    "def preprocess(folder):\n",
    "    no_of_candidates = []\n",
    "    no_of_paras = []\n",
    "    current_dir = directory + \"/\"+folder\n",
    "    base_cases = []\n",
    "    entailed_fragments = []\n",
    "    paragraphs = []\n",
    "    list_of_files = sorted(os.listdir(current_dir))\n",
    "    if folder == 'task2_train':\n",
    "        x=1\n",
    "    else:\n",
    "        x = 2\n",
    "    for i in range(len(list_of_files)-x):\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing \",i)\n",
    "        f = open(current_dir+'/'+list_of_files[i]+\"/base_case.txt\",\"r\")\n",
    "        g = open(current_dir+'/'+list_of_files[i]+\"/entailed_fragment.txt\",\"r\")\n",
    "        paragraph_filenames = sorted(os.listdir(current_dir+'/'+list_of_files[i]+\"/paragraphs\"))\n",
    "        no_of_candidates.append(len(paragraph_filenames))\n",
    "        a = f.read()\n",
    "        b = g.read()\n",
    "        base_cases.append(a)\n",
    "        entailed_fragments.append(b)\n",
    "        f.close()\n",
    "        g.close()\n",
    "        \n",
    "        arr = []\n",
    "        no_of_paras.append(len(paragraph_filenames))\n",
    "\n",
    "        for j in range(len(paragraph_filenames)):\n",
    "            g = open(current_dir+'/'+list_of_files[i]+\"/paragraphs/\"+paragraph_filenames[j],\"r\")\n",
    "            paragraphs.append(g.read())\n",
    "            g.close()\n",
    "    return base_cases,entailed_fragments,paragraphs,no_of_paras,no_of_candidates\n",
    "    \n",
    "#train_base_cases,train_entailed_fragments, train_paragraphs,train_no_of_paras = preprocess('task2_train')\n",
    "    \n",
    "test_base_cases,test_entailed_fragments, test_paragraphs,test_no_of_paras,no_of_candidates = preprocess('task2_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dict_words = set(nltk.corpus.words.words())\n",
    "def filt(case_docs):\n",
    "    filtered_words =[]\n",
    "    filtered_docs = [ '' for i in range(len(case_docs))]\n",
    "    lemma_docs = [ '' for i in range(len(case_docs))]\n",
    "    for i in range(len(case_docs)):\n",
    "        case_words = re.split(\"(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)\", case_docs[i])\n",
    "        filtered_word_list = [word.lower() for word in case_words if (( len(word) >= 3 and word.isalpha() and word.lower() not in stop_words ))  ]\n",
    "        filtered_words.append(filtered_word_list)\n",
    "        '''or ( len(word) <= 3 and word.isdigit())'''\n",
    "        for word in filtered_word_list:\n",
    "            filtered_docs[i] = filtered_docs[i] + word + \" \"\n",
    "            lemma_docs[i] = lemma_docs[i] + lemma.lemmatize(word) + \" \"\n",
    "    return lemma_docs\n",
    "test_entailed_fragments = filt(test_entailed_fragments)\n",
    "test_paragraphs = filt(test_paragraphs)\n",
    "test_base_cases = filt(test_base_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_entailed_fragments + test_paragraphs\n",
    "documents = test_entailed_fragments + test_paragraphs + test_base_cases\n",
    "def gen_labels(data):\n",
    "    labels = []\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        labels.append(\"base_case_\"+str(i+1))\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        for j in range(no_of_candidates[i]):\n",
    "            labels.append(\"candidate_\"+str(i+1)+\"_\"+str(j+1))\n",
    "    for i in range(len(test_entailed_fragments)):\n",
    "        labels.append(\"extra_\"+str(i+1))\n",
    "    return labels\n",
    "labels = gen_labels(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candidate_2_22',\n",
       " 'candidate_3_1',\n",
       " 'candidate_3_2',\n",
       " 'candidate_3_3',\n",
       " 'candidate_3_4',\n",
       " 'candidate_3_5',\n",
       " 'candidate_3_6',\n",
       " 'candidate_3_7',\n",
       " 'candidate_3_8',\n",
       " 'candidate_3_9',\n",
       " 'candidate_3_10',\n",
       " 'candidate_3_11',\n",
       " 'candidate_3_12',\n",
       " 'candidate_3_13',\n",
       " 'candidate_3_14',\n",
       " 'candidate_3_15',\n",
       " 'candidate_3_16',\n",
       " 'candidate_3_17',\n",
       " 'candidate_3_18',\n",
       " 'candidate_3_19',\n",
       " 'candidate_3_20',\n",
       " 'candidate_3_21',\n",
       " 'candidate_3_22',\n",
       " 'candidate_3_23',\n",
       " 'candidate_3_24',\n",
       " 'candidate_3_25',\n",
       " 'candidate_3_26',\n",
       " 'candidate_3_27',\n",
       " 'candidate_3_28',\n",
       " 'candidate_3_29',\n",
       " 'candidate_3_30',\n",
       " 'candidate_3_31',\n",
       " 'candidate_3_32',\n",
       " 'candidate_3_33',\n",
       " 'candidate_3_34',\n",
       " 'candidate_3_35',\n",
       " 'candidate_3_36',\n",
       " 'candidate_3_37',\n",
       " 'candidate_3_38',\n",
       " 'candidate_3_39',\n",
       " 'candidate_4_1',\n",
       " 'candidate_4_2',\n",
       " 'candidate_4_3',\n",
       " 'candidate_4_4',\n",
       " 'candidate_4_5',\n",
       " 'candidate_4_6',\n",
       " 'candidate_4_7',\n",
       " 'candidate_4_8',\n",
       " 'candidate_4_9',\n",
       " 'candidate_4_10']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[80:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim, logging\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dict_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=re.split('\\W+',doc), tags=[self.labels_list[idx]])\n",
    "iterator = DocIterator(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 19:44:20,320 : INFO : collecting all words and their counts\n",
      "2019-03-30 19:44:20,321 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-03-30 19:44:20,637 : INFO : collected 14802 word types and 1536 unique tags from a corpus of 1536 examples and 390500 words\n",
      "2019-03-30 19:44:20,638 : INFO : Loading a fresh vocabulary\n",
      "2019-03-30 19:44:20,679 : INFO : effective_min_count=2 retains 9829 unique words (66% of original 14802, drops 4973)\n",
      "2019-03-30 19:44:20,679 : INFO : effective_min_count=2 leaves 385527 word corpus (98% of original 390500, drops 4973)\n",
      "2019-03-30 19:44:20,740 : INFO : deleting the raw counts dictionary of 14802 items\n",
      "2019-03-30 19:44:20,742 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-03-30 19:44:20,744 : INFO : downsampling leaves estimated 293450 word corpus (76.1% of prior 385527)\n",
      "2019-03-30 19:44:20,799 : INFO : estimated required memory for 9829 words and 100 dimensions: 13699300 bytes\n",
      "2019-03-30 19:44:20,800 : INFO : resetting layer weights\n",
      "2019-03-30 19:44:20,972 : INFO : training model with 4 workers on 9829 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building vocabulary\n",
      "start training the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 19:44:21,614 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:21,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:21,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:21,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:21,628 : INFO : EPOCH - 1 : training on 390500 raw words (294999 effective words) took 0.6s, 455274 effective words/s\n",
      "2019-03-30 19:44:22,235 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:22,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:22,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:22,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:22,248 : INFO : EPOCH - 2 : training on 390500 raw words (295071 effective words) took 0.6s, 482908 effective words/s\n",
      "2019-03-30 19:44:22,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:22,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:22,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:22,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:22,906 : INFO : EPOCH - 3 : training on 390500 raw words (295070 effective words) took 0.6s, 461147 effective words/s\n",
      "2019-03-30 19:44:23,624 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:23,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:23,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:23,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:23,636 : INFO : EPOCH - 4 : training on 390500 raw words (295120 effective words) took 0.7s, 408033 effective words/s\n",
      "2019-03-30 19:44:24,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:24,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:24,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:24,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:24,254 : INFO : EPOCH - 5 : training on 390500 raw words (295196 effective words) took 0.6s, 494359 effective words/s\n",
      "2019-03-30 19:44:24,867 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:24,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:24,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:24,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:24,879 : INFO : EPOCH - 6 : training on 390500 raw words (295111 effective words) took 0.6s, 482584 effective words/s\n",
      "2019-03-30 19:44:25,495 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:25,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:25,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:25,507 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:25,508 : INFO : EPOCH - 7 : training on 390500 raw words (294799 effective words) took 0.6s, 484216 effective words/s\n",
      "2019-03-30 19:44:26,405 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:26,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:26,408 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:26,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:26,419 : INFO : EPOCH - 8 : training on 390500 raw words (294716 effective words) took 0.9s, 329400 effective words/s\n",
      "2019-03-30 19:44:27,043 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:27,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:27,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:27,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:27,056 : INFO : EPOCH - 9 : training on 390500 raw words (295019 effective words) took 0.6s, 469645 effective words/s\n",
      "2019-03-30 19:44:27,843 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:27,844 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:27,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:27,856 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:27,857 : INFO : EPOCH - 10 : training on 390500 raw words (294927 effective words) took 0.8s, 372829 effective words/s\n",
      "2019-03-30 19:44:28,464 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:28,465 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:28,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:28,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:28,478 : INFO : EPOCH - 11 : training on 390500 raw words (295067 effective words) took 0.6s, 494091 effective words/s\n",
      "2019-03-30 19:44:29,114 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:29,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:29,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:29,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:29,128 : INFO : EPOCH - 12 : training on 390500 raw words (294939 effective words) took 0.6s, 459124 effective words/s\n",
      "2019-03-30 19:44:29,724 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:29,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:29,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:29,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:29,736 : INFO : EPOCH - 13 : training on 390500 raw words (295138 effective words) took 0.6s, 493337 effective words/s\n",
      "2019-03-30 19:44:30,333 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:30,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:30,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:30,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:30,347 : INFO : EPOCH - 14 : training on 390500 raw words (295051 effective words) took 0.6s, 492463 effective words/s\n",
      "2019-03-30 19:44:30,938 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:30,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:30,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:30,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:30,951 : INFO : EPOCH - 15 : training on 390500 raw words (295006 effective words) took 0.6s, 494959 effective words/s\n",
      "2019-03-30 19:44:31,549 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:31,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:31,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:31,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:31,563 : INFO : EPOCH - 16 : training on 390500 raw words (295144 effective words) took 0.6s, 487871 effective words/s\n",
      "2019-03-30 19:44:32,176 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:32,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:32,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 19:44:32,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:32,190 : INFO : EPOCH - 17 : training on 390500 raw words (294939 effective words) took 0.6s, 479725 effective words/s\n",
      "2019-03-30 19:44:32,786 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:32,787 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:32,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:32,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:32,799 : INFO : EPOCH - 18 : training on 390500 raw words (294808 effective words) took 0.6s, 494916 effective words/s\n",
      "2019-03-30 19:44:33,399 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:33,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:33,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:33,411 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:33,413 : INFO : EPOCH - 19 : training on 390500 raw words (294983 effective words) took 0.6s, 489610 effective words/s\n",
      "2019-03-30 19:44:34,002 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:34,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:34,010 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:34,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:34,016 : INFO : EPOCH - 20 : training on 390500 raw words (295208 effective words) took 0.6s, 499886 effective words/s\n",
      "2019-03-30 19:44:34,606 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:34,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:34,615 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:34,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:34,625 : INFO : EPOCH - 21 : training on 390500 raw words (294854 effective words) took 0.6s, 490656 effective words/s\n",
      "2019-03-30 19:44:35,239 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:35,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:35,249 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:35,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:35,255 : INFO : EPOCH - 22 : training on 390500 raw words (294997 effective words) took 0.6s, 475940 effective words/s\n",
      "2019-03-30 19:44:35,870 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:35,872 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:35,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:35,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:35,885 : INFO : EPOCH - 23 : training on 390500 raw words (294970 effective words) took 0.6s, 476785 effective words/s\n",
      "2019-03-30 19:44:36,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:36,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:36,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:36,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:36,671 : INFO : EPOCH - 24 : training on 390500 raw words (295081 effective words) took 0.8s, 380083 effective words/s\n",
      "2019-03-30 19:44:37,498 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:37,500 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:37,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:37,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:37,512 : INFO : EPOCH - 25 : training on 390500 raw words (295187 effective words) took 0.8s, 356564 effective words/s\n",
      "2019-03-30 19:44:38,167 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:38,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:38,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:38,179 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:38,181 : INFO : EPOCH - 26 : training on 390500 raw words (295075 effective words) took 0.7s, 449613 effective words/s\n",
      "2019-03-30 19:44:38,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:38,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:38,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:38,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:38,816 : INFO : EPOCH - 27 : training on 390500 raw words (294941 effective words) took 0.6s, 474670 effective words/s\n",
      "2019-03-30 19:44:39,444 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:39,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:39,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:39,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:39,457 : INFO : EPOCH - 28 : training on 390500 raw words (294790 effective words) took 0.6s, 468226 effective words/s\n",
      "2019-03-30 19:44:40,066 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:40,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:40,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:40,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:40,087 : INFO : EPOCH - 29 : training on 390500 raw words (295157 effective words) took 0.6s, 481385 effective words/s\n",
      "2019-03-30 19:44:40,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:40,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:40,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:40,733 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:40,734 : INFO : EPOCH - 30 : training on 390500 raw words (294927 effective words) took 0.6s, 466164 effective words/s\n",
      "2019-03-30 19:44:41,345 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:41,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:41,349 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:41,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:41,358 : INFO : EPOCH - 31 : training on 390500 raw words (295054 effective words) took 0.6s, 481871 effective words/s\n",
      "2019-03-30 19:44:41,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:41,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:41,976 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:41,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:41,984 : INFO : EPOCH - 32 : training on 390500 raw words (294902 effective words) took 0.6s, 486748 effective words/s\n",
      "2019-03-30 19:44:42,591 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:42,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:42,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:42,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:42,602 : INFO : EPOCH - 33 : training on 390500 raw words (294985 effective words) took 0.6s, 483279 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 19:44:43,200 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:43,201 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:43,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:43,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:43,211 : INFO : EPOCH - 34 : training on 390500 raw words (294819 effective words) took 0.6s, 496582 effective words/s\n",
      "2019-03-30 19:44:43,839 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:43,841 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:43,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:43,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:43,851 : INFO : EPOCH - 35 : training on 390500 raw words (294967 effective words) took 0.6s, 472651 effective words/s\n",
      "2019-03-30 19:44:44,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:44,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:44,446 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:44,453 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:44,454 : INFO : EPOCH - 36 : training on 390500 raw words (295269 effective words) took 0.6s, 502744 effective words/s\n",
      "2019-03-30 19:44:45,065 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:45,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:45,068 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:45,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:45,078 : INFO : EPOCH - 37 : training on 390500 raw words (295066 effective words) took 0.6s, 481856 effective words/s\n",
      "2019-03-30 19:44:45,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:45,670 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:45,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:45,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:45,681 : INFO : EPOCH - 38 : training on 390500 raw words (294928 effective words) took 0.6s, 498268 effective words/s\n",
      "2019-03-30 19:44:46,328 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:46,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:46,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:46,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:46,341 : INFO : EPOCH - 39 : training on 390500 raw words (295147 effective words) took 0.6s, 460698 effective words/s\n",
      "2019-03-30 19:44:46,961 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:46,961 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:46,964 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:46,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:46,972 : INFO : EPOCH - 40 : training on 390500 raw words (294911 effective words) took 0.6s, 477486 effective words/s\n",
      "2019-03-30 19:44:47,594 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:47,596 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:47,602 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:47,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:47,608 : INFO : EPOCH - 41 : training on 390500 raw words (294928 effective words) took 0.6s, 469534 effective words/s\n",
      "2019-03-30 19:44:48,209 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:48,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:48,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:48,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:48,228 : INFO : EPOCH - 42 : training on 390500 raw words (295223 effective words) took 0.6s, 486475 effective words/s\n",
      "2019-03-30 19:44:49,211 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:49,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:49,216 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:49,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:49,228 : INFO : EPOCH - 43 : training on 390500 raw words (294782 effective words) took 1.0s, 298491 effective words/s\n",
      "2019-03-30 19:44:49,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:49,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:49,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:49,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:49,876 : INFO : EPOCH - 44 : training on 390500 raw words (295023 effective words) took 0.6s, 467902 effective words/s\n",
      "2019-03-30 19:44:50,479 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:50,481 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:50,483 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:50,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:50,491 : INFO : EPOCH - 45 : training on 390500 raw words (295017 effective words) took 0.6s, 493809 effective words/s\n",
      "2019-03-30 19:44:51,090 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:51,092 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:51,096 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:51,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:51,104 : INFO : EPOCH - 46 : training on 390500 raw words (295088 effective words) took 0.6s, 493017 effective words/s\n",
      "2019-03-30 19:44:51,744 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:51,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:51,748 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:51,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:51,758 : INFO : EPOCH - 47 : training on 390500 raw words (294949 effective words) took 0.6s, 461533 effective words/s\n",
      "2019-03-30 19:44:52,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:52,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:52,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:52,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:52,385 : INFO : EPOCH - 48 : training on 390500 raw words (295149 effective words) took 0.6s, 485524 effective words/s\n",
      "2019-03-30 19:44:53,002 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:53,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:53,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-30 19:44:53,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:53,014 : INFO : EPOCH - 49 : training on 390500 raw words (295210 effective words) took 0.6s, 486981 effective words/s\n",
      "2019-03-30 19:44:53,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-30 19:44:53,621 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-30 19:44:53,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-30 19:44:53,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-30 19:44:53,632 : INFO : EPOCH - 50 : training on 390500 raw words (295196 effective words) took 0.6s, 490804 effective words/s\n",
      "2019-03-30 19:44:53,633 : INFO : training on a 19525000 raw words (14750903 effective words) took 32.7s, 451649 effective words/s\n",
      "2019-03-30 19:44:53,634 : INFO : saving Doc2Vec object under doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model, separately None\n",
      "2019-03-30 19:44:53,789 : INFO : saved doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.66176462173462\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, alpha=0.025, min_alpha=0.001)\n",
    "model.build_vocab(iterator)\n",
    "\n",
    "print('done building vocabulary')\n",
    "print('start training the model')\n",
    "tic = time.time()\n",
    "model.train(iterator,total_examples=model.corpus_count, epochs = 50)\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "model.save(\"doc2vec_task_2_vec_100_window_5_test_set_minc_2_epoch_50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model = Doc2Vec.load(\"doc2vec_task_1_vec_150_window_10_test_set_minc_2_epoch_50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('candidate_4_1', 0.756076455116272),\n",
       " ('candidate_17_1', 0.7485694885253906),\n",
       " ('candidate_26_22', 0.7384686470031738),\n",
       " ('candidate_36_19', 0.7326580286026001),\n",
       " ('base_case_4', 0.7252758741378784),\n",
       " ('candidate_14_14', 0.7201084494590759),\n",
       " ('candidate_29_11', 0.7182266116142273),\n",
       " ('candidate_25_43', 0.7154099941253662),\n",
       " ('candidate_19_23', 0.7154039144515991),\n",
       " ('candidate_44_27', 0.7145591974258423),\n",
       " ('base_case_44', 0.7120709419250488),\n",
       " ('candidate_21_65', 0.711906909942627),\n",
       " ('candidate_25_46', 0.7117351293563843),\n",
       " ('candidate_38_25', 0.7116952538490295),\n",
       " ('candidate_28_45', 0.7114608287811279),\n",
       " ('base_case_31', 0.7110193967819214),\n",
       " ('candidate_3_23', 0.7100916504859924),\n",
       " ('candidate_24_39', 0.7073547840118408),\n",
       " ('candidate_1_8', 0.7066588997840881),\n",
       " ('base_case_28', 0.7065763473510742),\n",
       " ('candidate_37_2', 0.7058336734771729),\n",
       " ('candidate_32_30', 0.7055041790008545),\n",
       " ('base_case_3', 0.7045915126800537),\n",
       " ('candidate_3_39', 0.7038944363594055),\n",
       " ('candidate_36_32', 0.7033555507659912),\n",
       " ('candidate_30_31', 0.7025595903396606),\n",
       " ('base_case_19', 0.702468752861023),\n",
       " ('candidate_1_14', 0.7023906111717224),\n",
       " ('candidate_6_30', 0.7022256851196289),\n",
       " ('candidate_31_11', 0.697683572769165),\n",
       " ('candidate_25_34', 0.6945711970329285),\n",
       " ('candidate_31_31', 0.6940147876739502),\n",
       " ('candidate_26_1', 0.6931483745574951),\n",
       " ('candidate_12_24', 0.6924164891242981),\n",
       " ('base_case_23', 0.6922697424888611),\n",
       " ('candidate_12_25', 0.692119836807251),\n",
       " ('candidate_36_33', 0.6918689012527466),\n",
       " ('candidate_30_40', 0.6917334794998169),\n",
       " ('candidate_16_42', 0.6914256811141968),\n",
       " ('candidate_25_27', 0.691068172454834),\n",
       " ('candidate_44_47', 0.6904497742652893),\n",
       " ('base_case_38', 0.6901766657829285),\n",
       " ('candidate_31_5', 0.6884663105010986),\n",
       " ('candidate_5_61', 0.6874579787254333),\n",
       " ('candidate_28_46', 0.686721682548523),\n",
       " ('base_case_21', 0.6855669617652893),\n",
       " ('candidate_4_20', 0.6855126619338989),\n",
       " ('candidate_33_9', 0.6835297346115112),\n",
       " ('candidate_21_67', 0.6832762360572815),\n",
       " ('candidate_17_15', 0.6831570863723755)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(labels[0],topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2-1 8 IITP2d2v\n",
      "t2-2 4 IITP2d2v\n",
      "t2-3 39 IITP2d2v\n",
      "t2-4 20 IITP2d2v\n",
      "t2-5 61 IITP2d2v\n",
      "t2-6 30 IITP2d2v\n",
      "t2-7 25 IITP2d2v\n",
      "t2-8 26 IITP2d2v\n",
      "t2-9 15 IITP2d2v\n",
      "t2-10 4 IITP2d2v\n",
      "t2-11 1 IITP2d2v\n",
      "t2-12 24 IITP2d2v\n",
      "t2-13 5 IITP2d2v\n",
      "t2-14 14 IITP2d2v\n",
      "t2-15 19 IITP2d2v\n",
      "t2-16 59 IITP2d2v\n",
      "t2-17 15 IITP2d2v\n",
      "t2-18 18 IITP2d2v\n",
      "t2-19 23 IITP2d2v\n",
      "t2-20 28 IITP2d2v\n",
      "t2-21 62 IITP2d2v\n",
      "t2-22 21 IITP2d2v\n",
      "t2-23 17 IITP2d2v\n",
      "t2-24 20 IITP2d2v\n",
      "t2-25 43 IITP2d2v\n",
      "t2-26 22 IITP2d2v\n",
      "t2-27 18 IITP2d2v\n",
      "t2-28 45 IITP2d2v\n",
      "t2-29 11 IITP2d2v\n",
      "t2-30 31 IITP2d2v\n",
      "t2-31 31 IITP2d2v\n",
      "t2-32 30 IITP2d2v\n",
      "t2-33 9 IITP2d2v\n",
      "t2-34 4 IITP2d2v\n",
      "t2-35 7 IITP2d2v\n",
      "t2-36 1 IITP2d2v\n",
      "t2-37 22 IITP2d2v\n",
      "t2-38 25 IITP2d2v\n",
      "t2-39 12 IITP2d2v\n",
      "t2-40 32 IITP2d2v\n",
      "t2-41 15 IITP2d2v\n",
      "t2-42 1 IITP2d2v\n",
      "t2-43 6 IITP2d2v\n",
      "t2-44 27 IITP2d2v\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    #print(\"\\nPredicting  For \",i)\n",
    "    lst = []\n",
    "    for j in range(no_of_candidates[i]):\n",
    "        #lst.append((model.docvecs.similarity(labels[i],labels[285+i*200+j]),j+1))\n",
    "        lst.append((model.docvecs.similarity(labels[i],\"candidate_\"+str(i+1)+\"_\"+str(j+1)),j+1))\n",
    "    lst.sort(reverse=True)\n",
    "    cutoff = ((lst[0][0] + lst[1][0])/2)*0.90\n",
    "    for j in range(1):\n",
    "        if lst[j][0] > cutoff:\n",
    "            print(\"t2-\"+str(i+1),lst[j][1],\"IITP2d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "325,1486,855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "588/2421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "468/1486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = re.search(\"Baban\",\"I am Baban Gain Baban GAin\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
